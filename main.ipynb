{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import copy\n",
    "\n",
    "import nbimporter\n",
    "\n",
    "import utils.file_manager as fm\n",
    "import utils.pre_processing as ppc\n",
    "import utils.graph_plotter as gp\n",
    "\n",
    "import minetext.clustering.distance as distance\n",
    "import minetext.clustering.kmedoids as kmedoids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function that reads the export directories, at the export base directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_exports_directories(export_directories, exports_basedir):\n",
    "    exports = []\n",
    "    \n",
    "    for directory in exports_directories:\n",
    "        export_dirpath = os.path.join(exports_basedir, directory)\n",
    "        \n",
    "        # Lê os exports do diretório\n",
    "        export = fm.read_csv_directory(dirpath=export_dirpath, header=1)\n",
    "        \n",
    "        exports.append(export)\n",
    "        \n",
    "    return exports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function that do the pre-processing for each of the exports passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Realiza o pré-processamento e armazenamento dos exports\n",
    "def pre_processing(exports):\n",
    "    clean_exports = []\n",
    "    \n",
    "    for export in exports:\n",
    "        # Realiza o pré-processamento do conteúdo dos exports\n",
    "        clean_export = ppc.clean_reviews(export)\n",
    "        \n",
    "        clean_exports.append(clean_export)\n",
    "        \n",
    "    return clean_exports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function that saves the clean exports in CSV files, at the clean exports directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Realiza o pré-processamento e armazenamento dos exports\n",
    "def save_clean_exports(exports, export_directories, clean_exports_basedir):\n",
    "    # Colunas dos exports a serem consideradas\n",
    "    columns = [\"Content\", \"CleanContent\", \"Source\", \"Rating\"]\n",
    "    # Novos nomes de colunas para os exports pré-processados\n",
    "    header = [\"content\", \"clean_content\", \"source\", \"rating\"]\n",
    "    \n",
    "    fm.delete_directory(clean_exports_basedir)\n",
    "    \n",
    "    for directory, export in zip(exports_directories, exports):\n",
    "        filename = directory + \".csv\"\n",
    "\n",
    "        # Salva os exports já pré-processados em um arquivo .csv\n",
    "        fm.data_to_csv(data=export,\\\n",
    "                       dirpath=clean_exports_basedir,\\\n",
    "                       filename=filename,\\\n",
    "                       index=False,\\\n",
    "                       columns=columns,\\\n",
    "                       header=header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does the K-medoids clustering\n",
    "Also has the function to turn dataframe tuples into a list of dicts, with the passed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Função que transforma uma tupla do dataframe em um dict()\n",
    "# A função considera os atributos: Content, CleanContent, Source e Rating\n",
    "def dataframe_to_dicts(dataframe):\n",
    "    tuples = dataframe.itertuples() # Traz as linhas do dataframe em formato de tupla\n",
    "    columns = dataframe.keys() # Recupera o nome das columas do dataframe de exports\n",
    "    indexes = range(1, len(columns) + 1) # Cria um vetor com os indices a serem considerados\n",
    "    \n",
    "    dicts = []\n",
    "    \n",
    "    for tuple_t in tuples:\n",
    "        enumeration = None\n",
    "    \n",
    "        if not indexes:\n",
    "            enumeration = enumerate(columns)\n",
    "        else:\n",
    "            enumeration = zip(indexes, columns)\n",
    "\n",
    "        d = dict()\n",
    "\n",
    "        for index, column in enumeration:\n",
    "            d[column] = tuple_t[index]\n",
    "            \n",
    "        dicts.append(d)\n",
    "        \n",
    "    return dicts\n",
    "\n",
    "def clustering_kmedoids(export, distance_calculator, k_min=3, k_max=15, iteractions=20, n=20):\n",
    "    # Dict que armazenará as reviews como documentos a serem clusterizados\n",
    "    reviews = dict()\n",
    "    reviews[\"reviews\"] = dataframe_to_dicts(export)\n",
    "    \n",
    "    result = dict()\n",
    "    result[\"k\"] = []        \n",
    "    result[\"sse\"] = []\n",
    "    result[\"iteraction\"] = []\n",
    "    result[\"clusters\"] = []\n",
    "    result[\"most_similar\"] = []\n",
    "\n",
    "    for k in range(k_min, k_max + 1):\n",
    "        min_sse = float(\"inf\")\n",
    "        min_itr = None        \n",
    "        min_clusters = None\n",
    "        min_most_similar = None\n",
    "\n",
    "        for itr in range(1, iteractions + 1):\n",
    "            # Simulação de passagem de parâmetro po cópia.\n",
    "            # Necessário pois é preciso uma cópia para cada instância do kmedoids, para evitar inconsistências.\n",
    "            documents = copy.deepcopy(reviews[\"reviews\"])\n",
    "\n",
    "            kmedoids_instance = kmedoids.Kmedoids(k=k,\\\n",
    "                                                  documents=documents,\\\n",
    "                                                  distance_calculator=distance_calculator,\\\n",
    "                                                  collection_field=\"reviews\", text_field_name=\"clean_content\")\n",
    "\n",
    "            kmedoids_instance.clustering()                \n",
    "\n",
    "            sse = kmedoids_instance.calculate_sse()\n",
    "\n",
    "            if sse < min_sse:\n",
    "                min_itr = itr\n",
    "                min_sse = sse\n",
    "                min_clusters = copy.deepcopy(kmedoids_instance.get_clusters())\n",
    "                min_most_similar = copy.deepcopy(kmedoids_instance.n_most_similar_for_clusters_medoid(n))\n",
    "\n",
    "        result[\"k\"].append(k)\n",
    "        result[\"sse\"].append(min_sse)\n",
    "        result[\"iteraction\"].append(min_itr)\n",
    "        result[\"clusters\"].append(min_clusters)\n",
    "        result[\"most_similar\"].append(min_most_similar)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function that reads the clean reviews CSV files in the clean exports directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lê os diretórios com os exports pré-processados\n",
    "def read_clean_exports_directories(export_directories, clean_exports_basedir):\n",
    "    exports = []\n",
    "    \n",
    "    for directory in export_directories:\n",
    "        export_filename = directory + \".csv\"\n",
    "\n",
    "        export = fm.read_csv_file(dirpath=clean_exports_basedir, filename=export_filename)\n",
    "        \n",
    "        exports.append(export)\n",
    "        \n",
    "    return exports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saves the clustering results in the results directory\n",
    "Saves overall results, clusters generated, most similar reviews and word clouds of each cluster, for all the K values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_clustering_result(result, directory, results_basedir, collection_field=\"reviews\", text_field_name=\"clean_content\", overall_results_filename=\"clustering_overall_results.csv\"):\n",
    "        result_directory = os.path.join(results_basedir, directory)\n",
    "        \n",
    "        fm.delete_directory(result_directory)\n",
    "        \n",
    "        overall_results_columns = [\"k\", \"iteraction\", \"sse\"]\n",
    "        \n",
    "        # Save overall clustering results\n",
    "        fm.overall_results_to_csv(results=result,\\\n",
    "                                 dirpath=result_directory,\\\n",
    "                                 filename=overall_results_filename,\\\n",
    "                                 columns=overall_results_columns)\n",
    "        \n",
    "        clusters_results_columns = [\"content\", \"clean_content\", \"cluster\", \"rating\"]\n",
    "        \n",
    "        # Save cluster contents\n",
    "        fm.clusters_to_csv(result=result,\\\n",
    "                           dirpath=result_directory,\\\n",
    "                           collection_field=\"reviews\",\\\n",
    "                           columns=clusters_results_columns)\n",
    "        \n",
    "        # Save the clusters' most similar documents\n",
    "        fm.most_similar_to_csv(result=result, dirpath=result_directory)\n",
    "        \n",
    "        # Save the clusters' wordclouds\n",
    "        fm.clusters_to_wordcloud(result=result,\\\n",
    "                                 dirpath=result_directory,\\\n",
    "                                 collection_field=collection_field,\\\n",
    "                                 text_field_name=text_field_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(results_basedir, directory, k_min=3, k_max=20):\n",
    "    results_dirpath = os.path.join(results_basedir, directory)\n",
    "    \n",
    "    overall_results = fm.overall_results_from_csv(results_dirpath)\n",
    "    gp.plot_overall_clustering_results(overall_results, results_dirpath)\n",
    "    \n",
    "    clusters = fm.clusters_dataframe_from_csv(results_dirpath)\n",
    "\n",
    "    for k in range(k_min, k_max + 1):\n",
    "        k_subdir = os.path.join(results_dirpath, 'clustering', 'K_' + str(k))\n",
    "        \n",
    "        itr_subdirs = os.listdir(k_subdir)\n",
    "        \n",
    "        for itr_subdir in itr_subdirs:\n",
    "            graphs_dirpath = os.path.join(k_subdir, itr_subdir, \"graphs\")\n",
    "            \n",
    "            gp.plot_clusters_mean_ratings(clusters, k=k, dirpath=graphs_dirpath) \n",
    "            gp.plot_clusters_document_count(clusters, k=k, dirpath=graphs_dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application variables (mostly workspace directories)\n",
    "\n",
    "Also defines the exports directories considered in the application execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Diretórios base\n",
    "data_basedir = os.path.join(os.getcwd(), \"data\") # Diretório geral de dados\n",
    "\n",
    "exports_basedir = os.path.join(data_basedir, \"exports\") # Subdiretório dos exports\n",
    "\n",
    "clean_exports_basedir = os.path.join(data_basedir, \"clean_exports\") # Subdiretório os exports pré-processados\n",
    "results_jaccard_basedir = os.path.join(data_basedir, \"results\", \"jaccard\") # Subdiretório dos resultados\n",
    "results_levenshtein_basedir = os.path.join(data_basedir, \"results\", \"levenshtein\") # Subdiretório dos resultados\n",
    "\n",
    "# Lista dos nomes dos diretórios que contém os exports\n",
    "exports_directories = [\"facebook\", \"telegram\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reads the exports directories, and pre-process the reviews, and saves them in the results directory for each export directory defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_exports = read_exports_directories(exports_directories, exports_basedir)\n",
    "\n",
    "clean_exports = pre_processing(raw_exports)\n",
    "\n",
    "save_clean_exports(clean_exports, exports_directories, clean_exports_basedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runs the K-medoids clustering algorithm using the Jaccard distance calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"K-medoids with Jaccard distance started.\")\n",
    "exports = read_clean_exports_directories(exports_directories, clean_exports_basedir)\n",
    "\n",
    "for directory, export in zip(exports_directories, exports):\n",
    "    print(\"Directory: \", directory)\n",
    "    result = clustering_kmedoids(export, distance.JaccardCalculatorDistance(), k_min=3, k_max=80, iteractions=40, n=50)\n",
    "    \n",
    "    save_clustering_result(result, directory, results_jaccard_basedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runs the K-medoids clustering algorithm using the Levenshtein distance calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"K-medoids with Levenshtein distance started.\")\n",
    "exports = read_clean_exports_directories(exports_directories, clean_exports_basedir)\n",
    "\n",
    "for directory, export in zip(exports_directories, exports):\n",
    "    print(\"Directory: \", directory)\n",
    "    result = clustering_kmedoids(export, distance.LevenshteinCalculator(), k_min=3, k_max=30, iteractions=20, n=50)\n",
    "    \n",
    "    save_clustering_result(result, directory, results_levenshtein_basedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for results_basedir, k_min, k_max in zip([results_jaccard_basedir, results_levenshtein_basedir], [3, 3], [80, 30]):\n",
    "    for directory in exports_directories:\n",
    "        plot_graphs(results_basedir, directory, k_min=k_min, k_max=k_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for results_basedir in [results_jaccard_basedir, results_levenshtein_basedir]:\n",
    "    for directory in exports_directories:\n",
    "        result_dirpath = os.path.join(results_basedir, directory)\n",
    "        \n",
    "        overall_results = fm.overall_results_from_csv(result_dirpath)\n",
    "    \n",
    "        print(gp.calculate_mininal_difference(overall_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
